{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w8_practice_lstm_tf.ipynb","provenance":[{"file_id":"1_jvSnxNR98t3Af5O4fAyrtZBniRI08KI","timestamp":1598043679928},{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/master/week08_pomdp/practice_tensorflow.ipynb","timestamp":1597868587559}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hxrhkGBVY4UN","colab_type":"text"},"source":["### Deep Kung-Fu with advantage actor-critic\n","\n","In this notebook you'll build a deep reinforcement learning agent for Atari [Kung-Fu Master](https://gym.openai.com/envs/KungFuMaster-v0/) that uses a recurrent neural net.\n","\n","![https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png](https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png)"]},{"cell_type":"code","metadata":{"id":"zRUw7nQHY4UO","colab_type":"code","colab":{}},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    # https://github.com/yandexdataschool/Practical_RL/issues/256\n","    !pip uninstall tensorflow --yes\n","    !pip uninstall keras --yes\n","    !pip install tensorflow-gpu==1.13.1\n","    !pip install keras==2.2.4\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n","\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week08_pomdp/atari_util.py\n","        \n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhkyrISI5lA5","colab_type":"code","colab":{}},"source":["\"\"\"\n","A thin wrapper for openAI gym environments that maintains a set of parallel games and has a method to generate\n","interaction sessions given agent one-step applier function.\n","\"\"\"\n","\n","import numpy as np\n","\n","# A whole lot of space invaders\n","\n","\n","class EnvPool(object):\n","    def __init__(self, agent, make_env, n_parallel_games=1):\n","        \"\"\"\n","        A special class that handles training on multiple parallel sessions\n","        and is capable of some auxilary actions like evaluating agent on one game session (See .evaluate()).\n","\n","        :param agent: Agent which interacts with the environment.\n","        :param make_env: Factory that produces environments OR a name of the gym environment.\n","        :param n_games: Number of parallel games. One game by default.\n","        :param max_size: Max pool size by default (if appending sessions). By default, pool is not constrained in size.\n","        \"\"\"\n","        # Create atari games.\n","        self.agent = agent\n","        self.make_env = make_env\n","        self.envs = [self.make_env() for _ in range(n_parallel_games)]\n","\n","        # Initial observations.\n","        self.prev_observations = [env.reset() for env in self.envs]\n","\n","        # Agent memory variables (if you use recurrent networks).\n","        self.prev_memory_states = agent.get_initial_state(n_parallel_games)\n","\n","        # Whether particular session has just been terminated and needs\n","        # restarting.\n","        self.just_ended = [False] * len(self.envs)\n","\n","\n","    def interact(self, n_steps=100, verbose=False):\n","        \"\"\"Generate interaction sessions with ataries (openAI gym atari environments)\n","        Sessions will have length n_steps. Each time one of games is finished, it is immediately getting reset\n","        and this time is recorded in is_alive_log (See returned values).\n","\n","        :param n_steps: Length of an interaction.\n","        :returns: observation_seq, action_seq, reward_seq, is_alive_seq\n","        :rtype: a bunch of tensors [batch, tick, ...]\n","        \"\"\"\n","\n","        def env_step(i, action):\n","            if not self.just_ended[i]:\n","                new_observation, cur_reward, is_done, info = self.envs[i].step(action)\n","                if is_done:\n","                    # Game ends now, will finalize on next tick.\n","                    self.just_ended[i] = True\n","\n","                # note: is_alive=True in any case because environment is still\n","                # alive (last tick alive) in our notation.\n","                return new_observation, cur_reward, True, info\n","            else:\n","                # Reset environment, get new observation to be used on next\n","                # tick.\n","                new_observation = self.envs[i].reset()\n","\n","                # Reset memory for new episode.\n","                initial_memory_state = self.agent.get_initial_state(\n","                    batch_size=1)\n","                for m_i in range(len(new_memory_states)):\n","                    new_memory_states[m_i][i] = initial_memory_state[m_i][0]\n","\n","                if verbose:\n","                    print(\"env %i reloaded\" % i)\n","\n","                self.just_ended[i] = False\n","\n","                return new_observation, 0, False, {'end': True}\n","\n","        history_log = []\n","\n","        last_prev_mem_state = self.prev_memory_states\n","\n","        for i in range(n_steps):\n","            dropout = False if i <= n_steps - 2 else True\n","            new_memory_states, readout = self.agent.step(self.prev_memory_states, \n","                                                         self.prev_observations,\n","                                                         dropout)\n","            sampled_actions = self.agent.sample_actions(readout)\n","\n","            new_observations, cur_rewards, is_alive, infos = zip(\n","                *map(env_step, range(len(self.envs)), sampled_actions))\n","\n","            # Append data tuple for this tick.\n","            history_log.append(\n","                (self.prev_observations, sampled_actions, cur_rewards, is_alive))\n","\n","            last_prev_mem_state = self.prev_memory_states\n","            self.prev_observations = new_observations\n","            self.prev_memory_states = new_memory_states\n","\n","        # add last observation\n","        #dummy_actions = [0] * len(self.envs)\n","        #dummy_rewards = [0] * len(self.envs)\n","        #dummy_mask = [1] * len(self.envs)\n","        #history_log.append(\n","        #    (self.prev_observations,\n","        #     dummy_actions,\n","        #     dummy_rewards,\n","        #     dummy_mask))\n","\n","        # cast to numpy arrays, \n","        # transpose from [time, batch, ...] to [batch, time, ...]\n","        history_log = [\n","            np.array(tensor).swapaxes(0, 1)\n","            for tensor in zip(*history_log)\n","        ]\n","        observation_seq, action_seq, reward_seq, is_alive_seq = history_log\n","\n","        return observation_seq, action_seq, reward_seq, is_alive_seq, last_prev_mem_state\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cm35vEADY4UT","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from IPython.display import display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4pI8-QsiY4UW","colab_type":"text"},"source":["For starters, let's take a look at the game itself:\n","\n","* Image resized to 42x42 and converted to grayscale to run faster\n","* Agent sees last 4 frames of game to account for object velocity"]},{"cell_type":"code","metadata":{"id":"Qg8qz_iMY4UX","colab_type":"code","colab":{}},"source":["import gym\n","from atari_util import PreprocessAtari\n","\n","def make_env():\n","    env = gym.make(\"KungFuMasterDeterministic-v0\")\n","    env = PreprocessAtari(\n","        env, height=42, width=42,\n","        crop=lambda img: img[60:-30, 5:],\n","        dim_order='tensorflow',\n","        color=False, n_frames=4)\n","    return env\n","\n","env = make_env()\n","\n","obs_shape = env.observation_space.shape\n","n_actions = env.action_space.n\n","\n","print(\"Observation shape:\", obs_shape)\n","print(\"Num actions:\", n_actions)\n","print(\"Action names:\", env.env.env.get_action_meanings())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z658owR2Y4Uc","colab_type":"code","colab":{}},"source":["s = env.reset()\n","for _ in range(100):\n","    s, _, _, _ = env.step(env.action_space.sample())\n","\n","plt.title('Game image')\n","plt.imshow(env.render('rgb_array'))\n","plt.show()\n","\n","plt.title('Agent observation (4-frame buffer)')\n","plt.imshow(s.transpose([0, 2, 1]).reshape([42,-1]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-iqdJDMY4Uf","colab_type":"text"},"source":["### Simple agent for fully-observable MDP\n","\n","Here's a code for an agent that only uses feedforward layers. Please read it carefully: you'll have to extend it later!"]},{"cell_type":"code","metadata":{"id":"wiTI0SnkY4Ug","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from keras.layers import Conv2D, Dense, Flatten, Dropout\n","from tensorflow.nn.rnn_cell import LSTMCell, LSTMStateTuple\n","\n","\n","tf.reset_default_graph()\n","sess = tf.InteractiveSession()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ff74wi7RY4Uj","colab_type":"code","colab":{}},"source":["class SimpleRecurrentAgent:\n","    def __init__(self, name, obs_shape, n_actions, reuse=False):\n","        \"\"\"A simple actor-critic agent\"\"\"\n","\n","        with tf.variable_scope(name, reuse=reuse):\n","            # Note: number of units/filters is arbitrary, you can and should change it at your will\n","             # Note: number of units/filters is arbitrary, you can and should change it at your will\n","            self.conv0 = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')\n","            self.conv1 = Conv2D(64, (3, 3), strides=(2, 2), activation='relu')\n","            self.conv2 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n","\n","            self.flatten = Flatten()\n","            self.hid     = Dense(512, activation='relu')\n","            \n","            # Actor: pi(a|s)\n","            self.logits  = Dense(n_actions) \n","            \n","            # Critic: State Values\n","            self.state_value = Dense(1)\n","            \n","            # Recurrent Layer\n","            self.hid_size  = 512\n","            self.rnn0 = LSTMCell(self.hid_size, state_is_tuple = True)\n","            self.dropout = Dropout(9/10)\n","\n","            # prepare a graph for agent step\n","            initial_state_c = tf.placeholder(dtype=tf.float32, \n","                                             shape=[None, self.hid_size],\n","                                             name=\"init_state_c\")\n","            \n","            initial_state_h = tf.placeholder(dtype=tf.float32, \n","                                             shape=[None, self.hid_size],\n","                                             name=\"init_state_h\")\n","            \n","            self.prev_state_placeholder = LSTMStateTuple(initial_state_c, initial_state_h)\n","\n","            self.obs_t = tf.placeholder(tf.float32, [None, ] + list(obs_shape))\n","\n","            self.next_state, self.agent_outputs = self.symbolic_step(self.prev_state_placeholder,\n","                                                                     self.obs_t)\n","            \n","            c = self.dropout(self.next_state[0])\n","            h = self.dropout(self.next_state[1])\n","            self.next_state_dropout = LSTMStateTuple(c, h)\n","\n","\n","    def symbolic_step(self, prev_state, obs_t):\n","        \"\"\"Takes agent's previous step and observation, returns next state and whatever it needs to learn (tf tensors)\"\"\"\n","        nn = self.conv0(obs_t)\n","        nn = self.conv1(nn)\n","        nn = self.conv2(nn)\n","        nn = self.flatten(nn)\n","        nn = self.hid(nn)\n","        \n","        # Apply recurrent neural net for one step here.\n","        # The recurrent cell should take the last feedforward dense layer as input.\n","        batch_ones = tf.ones(tf.shape(obs_t)[0])\n","        new_out, new_state_ch = tf.nn.dynamic_rnn(self.rnn0, nn[:,None],\n","                                                  initial_state = prev_state,\n","                                                  sequence_length = batch_ones)\n","        \n","        logits      = self.logits(new_out[:,0])\n","        state_value = self.state_value(new_out[:,0])\n","\n","        return new_state_ch, (logits, state_value)\n","\n","\n","    def get_initial_state(self, batch_size):\n","        # LSTMStateTuple([batch_size x hid_size], [batch_size x hid_size]]\n","        a = np.zeros([batch_size, self.hid_size], dtype=np.float32)\n","        return LSTMStateTuple(a, a)\n","\n","\n","    # Instantiation\n","    def step(self, prev_state, obs_t, dropout=False):\n","        \"\"\"Same as symbolic state except it operates on numpy arrays\"\"\"\n","        sess = tf.get_default_session()\n","        \n","        feed_dict = {self.obs_t: obs_t,\n","                     self.prev_state_placeholder: prev_state}\n","        \n","        state_ph = self.next_state if dropout==False else self.next_state_dropout\n","        return sess.run([state_ph, self.agent_outputs], feed_dict)\n","\n","\n","    def sample_actions(self, agent_outputs):\n","        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n","        logits, state_values = agent_outputs\n","        policy = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n","\n","        return [np.random.choice(len(p), p=p) for p in policy]\n","\n","    \n","    def get_state_values(self, prev_state, obs_t):\n","        sess = tf.get_default_session()\n","        \n","        feed_dict = {self.obs_t: obs_t,\n","                     self.prev_state_placeholder: prev_state}\n","        \n","        return sess.run(self.agent_outputs[1], feed_dict)\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKfAXDe1Y4Un","colab_type":"code","colab":{}},"source":["n_parallel_games = 10\n","rollout_length   = 30\n","gamma = 0.99\n","\n","agent = SimpleRecurrentAgent('agent_with_memory', obs_shape, n_actions)\n","sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjgYtYdwY4Uq","colab_type":"code","colab":{}},"source":["state = [env.reset()]\n","_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n","print(\"action logits:\\n\", logits)\n","print(\"state values:\\n\", value)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"klqvnKWdY4Us","colab_type":"text"},"source":["### Let's play!\n","Let's build a function that measures agent's average reward."]},{"cell_type":"code","metadata":{"id":"ln4WuILsY4Ut","colab_type":"code","colab":{}},"source":["def evaluate(agent, env, n_games=1):\n","    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n","\n","    game_rewards = []\n","    for _ in range(n_games):\n","        # initial observation and memory\n","        observation = env.reset()\n","        prev_memories = agent.get_initial_state(1)\n","\n","        total_reward = 0\n","        while True:\n","            new_memories, readouts = agent.step(\n","                prev_memories, observation[None, ...])\n","            action = agent.sample_actions(readouts)\n","\n","            observation, reward, done, info = env.step(action[0])\n","\n","            total_reward += reward\n","            prev_memories = new_memories\n","            if done:\n","                break\n","\n","        game_rewards.append(total_reward)\n","    return game_rewards"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PArnjUsxI4Fq","colab_type":"code","colab":{}},"source":["#import gym.wrappers\n","\n","#with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n","#    rewards = evaluate(agent, env_monitor, n_games=3)\n","\n","#print(rewards)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrmfSJAEY4Uy","colab_type":"code","colab":{}},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","#from pathlib import Path\n","#from IPython.display import HTML\n","\n","#video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","#HTML(\"\"\"\n","#<video width=\"640\" height=\"480\" controls>\n","#  <source src=\"{}\" type=\"video/mp4\">\n","#</video>\n","#\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ff_83RwqY4U0","colab_type":"text"},"source":["### Training on parallel games\n","\n","We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n","![img](https://s7.postimg.cc/4y36s2b2z/env_pool.png)"]},{"cell_type":"code","metadata":{"id":"8Q77ZJKGY4U1","colab_type":"code","colab":{}},"source":["pool = EnvPool(agent, make_env, n_parallel_games)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WF9OpsgRY4U3","colab_type":"code","colab":{}},"source":["# for each of n_parallel_games, take \"rollout_length\" steps\n","rollout_obs, rollout_actions, rollout_rewards, rollout_mask, last_prev_mem_state = pool.interact(rollout_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HQ7gxu3Y4U8","colab_type":"code","colab":{}},"source":["print(\"Actions shape:\", rollout_actions.shape)\n","print(\"Rewards shape:\", rollout_rewards.shape)\n","print(\"Mask shape:\", rollout_mask.shape)\n","print(\"Observations shape: \", rollout_obs.shape)\n","print(\"Last Previous Memory State: \", (last_prev_mem_state[0].shape, last_prev_mem_state[1].shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mhyVE0AzY4VA","colab_type":"text"},"source":["# Actor-critic objective\n","\n","Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n","\n","\n","Our loss consists of three components:\n","\n","* __The policy \"loss\"__\n"," $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n","  * This function has no meaning in and of itself, but it was built such that\n","  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n","  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n","  \n","  \n","* __The value \"loss\"__\n","  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n","  * Ye Olde TD_loss from q-learning and alike\n","  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n","\n","\n","* __Entropy Regularizer__\n","  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n","  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n","  prematurely (a.k.a. exploration)\n","  \n","  \n","So we optimize a linear combination of $L_{td} - \\hat J -H$\n","\n","\n","__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n","  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n","  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n","  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n","\n","\n","__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."]},{"cell_type":"code","metadata":{"id":"d_98omkZY4VA","colab_type":"code","colab":{}},"source":["# [batch, time, h, w, c]\n","observations_ph       = tf.placeholder('float32', [None, None,] + list(obs_shape))\n","sampled_actions_ph    = tf.placeholder('int32',   (None, None,))\n","mask_ph               = tf.placeholder('float32', (None, None,))\n","\n","rewards_ph            = tf.placeholder('float32', (None, None,))\n","cumulative_rewards_ph = tf.placeholder('float32', (None, None,))\n","\n","initial_memory_ph = agent.prev_state_placeholder\n","\n","# get new_state, (actor->logits, critic->state_value)\n","next_state, dummy_outputs = agent.symbolic_step(initial_memory_ph,\n","                                                observations_ph[:, 0])\n","print(\"dummy_outputs:\", dummy_outputs,'\\n')\n","\n","next_memory_seq, outputs_seq = tf.scan(\n","    lambda stack, obs_t: agent.symbolic_step(stack[0], obs_t),\n","    # return new_state_ch, (logits, state_value) \n","    initializer = (initial_memory_ph, dummy_outputs),\n","    elems = tf.transpose(observations_ph, [1, 0, 2, 3, 4])\n","    # elem.shape = [time, batch, h, w, c]\n",")\n","print(\"next_memory_seq\", next_memory_seq,'\\n')\n","print(\"outputs_seq:\", outputs_seq,'\\n')\n","\n","# from [time, batch] back to [batch, time]\n","outputs_seq = [tf.transpose(\n","    tensor, [1, 0] + list(range(2, tensor.shape.ndims))) for tensor in outputs_seq]\n","print(\"outputs_seq:\", outputs_seq)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yf8tHYyRY4VE","colab_type":"code","colab":{}},"source":["# actor-critic losses\n","# actor  -> logits, with shape: [batch, time, n_actions]\n","# critic -> states, with shape: [batch, time, 1]\n","logits_seq, state_values_seq = outputs_seq\n","\n","logprobs_seq = tf.nn.log_softmax(logits_seq)\n","logp_actions = tf.reduce_sum(logprobs_seq * tf.one_hot(sampled_actions_ph, n_actions),\n","                             axis=-1)[:, :-1]\n","\n","current_rewards = rewards_ph[:, :-1] / 100.\n","current_state_values = state_values_seq[:, :-1, 0]\n","\n","next_state_values = state_values_seq[:, 1:, 0] * mask_ph[:, :-1]\n","\n","\n","# policy gradient\n","# compute 1-step advantage using current_rewards, current_state_values and next_state_values\n","advantage = cumulative_rewards_ph[:, :-1] - current_state_values\n","assert advantage.shape.ndims == 2\n","\n","# compute policy entropy given logits_seq. Mind the sign!\n","policy  = tf.nn.softmax(logits_seq, axis=-1)\n","entropy = - tf.reduce_sum(policy * logprobs_seq, axis=-1)\n","assert entropy.shape.ndims == 2\n","\n","actor_loss = - tf.reduce_mean(logp_actions * tf.stop_gradient(advantage))\n","actor_loss -= 0.001 * tf.reduce_mean(entropy)\n","\n","# Prepare Temporal Difference error (States)\n","target_state_values = current_rewards + gamma * next_state_values\n","critic_loss = tf.reduce_mean(\n","    (current_state_values - tf.stop_gradient(target_state_values))**2)\n","\n","\n","train_step = tf.train.AdamOptimizer(1e-5).minimize(actor_loss + critic_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cm6ND-61Y4VH","colab_type":"code","colab":{}},"source":["sess.run(tf.global_variables_initializer())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNl_YI7JY4VL","colab_type":"text"},"source":["# Train \n","\n","just run train step and see if agent learns any better"]},{"cell_type":"code","metadata":{"id":"FengAe_yPtZg","colab_type":"code","colab":{}},"source":["def acc_rewards(rewards, last_state_values, gamma=0.99):\n","        # rewards at each step [batch, time]\n","        # in a phase, last previous memory state [batch, state_dim]\n","        # discount for reward\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    curr_rewards = rewards / 100.\n","    b_size, time = rewards.shape\n","    \n","    acc_reward = np.zeros((b_size, time), dtype='float32')\n","    acc_reward[:,time-1] = last_state_values\n","\n","    for i in reversed(np.arange(time-1)):\n","        acc_reward[:,i] = curr_rewards[:,i] + gamma * acc_reward[:,i+1]\n","        \n","    return acc_reward"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAkqEf_BY4VM","colab_type":"code","colab":{}},"source":["def sample_batch(rollout_length=rollout_length):\n","    rollout_obs, rollout_actions, rollout_rewards, rollout_mask, prev_state = pool.interact(rollout_length)\n","\n","    last_state_values = agent.get_state_values(prev_state, rollout_obs[:,-1])\n","    \n","    rollout_cumulative_rewards = acc_rewards(rollout_rewards, last_state_values[:,0])\n","    \n","    feed_dict = {\n","        initial_memory_ph: pool.prev_memory_states,\n","        observations_ph: rollout_obs,\n","        sampled_actions_ph: rollout_actions,\n","        mask_ph: rollout_mask,\n","        rewards_ph: rollout_rewards,\n","        cumulative_rewards_ph: rollout_cumulative_rewards,\n","    }\n","\n","    return feed_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uoSvv_ewY4VO","colab_type":"code","colab":{}},"source":["from IPython.display import clear_output\n","\n","from tqdm import trange\n","from pandas import DataFrame\n","\n","moving_average = lambda x, **kw: DataFrame(\n","    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n","\n","rewards_history = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_3kOalqY4VR","colab_type":"code","colab":{}},"source":["iters = 301\n","for i in trange(iters):\n","    sess.run(train_step, sample_batch())\n","\n","    if i % 100 == 0:\n","        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n","        clear_output(True)\n","        plt.plot(rewards_history, label='rewards')\n","        plt.plot(moving_average(np.array(rewards_history),\n","                                span=rollout_length), label='rewards ewma@'+str(rollout_length))\n","        plt.legend()\n","        plt.show()\n","        if rewards_history[-1] >= 20000:\n","            print(\"Your agent has just passed the minimum homework threshold\")\n","            break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Il8-jD91Y4VV","colab_type":"text"},"source":["### \"Final\" evaluation"]},{"cell_type":"code","metadata":{"id":"1rPp_T9yB91q","colab_type":"code","colab":{}},"source":["import gym.wrappers\n","\n","with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n","    final_rewards = evaluate(agent, env_monitor, n_games=n_parallel_games)\n","\n","print(\"Final mean reward\", np.mean(final_rewards))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBeg64nWY4VY","colab_type":"code","colab":{}},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6t_y0TcOY4Va","colab_type":"text"},"source":["### POMDP setting\n","\n","The Atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory. \n","\n","Let's design another agent that has a recurrent neural net memory to solve this.\n","\n","__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."]},{"cell_type":"markdown","metadata":{"id":"fJgo_LgQY4Vg","colab_type":"text"},"source":["### Now let's train it!"]},{"cell_type":"code","metadata":{"id":"cyzuw5WcY4Vh","colab_type":"code","colab":{}},"source":["# A whole lot of your code here: train the new agent with GRU memory.\n","# - create pool\n","# - write loss functions and training op\n","# - train\n","# You can reuse most of the code with zero to few changes"],"execution_count":null,"outputs":[]}]}