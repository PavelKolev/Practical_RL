{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"w6_reinforce_a2c_tf.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/master/week06_policy_based/reinforce_tensorflow.ipynb","timestamp":1596730263284}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"l-1PFyXIifvV","colab_type":"text"},"source":["# REINFORCE in TensorFlow (3 pts)\n","\n","Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"a6iK412GifwJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1596823591196,"user_tz":-120,"elapsed":17189,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"54c47b18-f62c-4178-c125-98c3dfed9a6d"},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    #%tensorflow_version 1.x\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Selecting previously unselected package xvfb.\n","(Reading database ... 144487 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d9URIMyYifwX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823591202,"user_tz":-120,"elapsed":17168,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9aPQ1eYAifwo","colab_type":"text"},"source":["A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."]},{"cell_type":"code","metadata":{"id":"5EicySV5ifwq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1596823593272,"user_tz":-120,"elapsed":19221,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"3294432c-b303-49e7-b450-35b483cf16d9"},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f679e5f9828>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"YZz0Eg1foiyA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1596823593274,"user_tz":-120,"elapsed":19202,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"9110a696-20c4-4062-842d-2c3bae1c0058"},"source":["print(n_actions)\n","print(state_dim)\n","print(state_dim[0])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["2\n","(4,)\n","4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SplNCokXifw4","colab_type":"text"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"CBwkXjcZifw5","colab_type":"text"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"c52-P5VKifw7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1596823600080,"user_tz":-120,"elapsed":25987,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"d58c3fac-7a64-486b-9374-4e9009be5d85"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","sess = tf.InteractiveSession()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xM3f8EJMifxF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823600084,"user_tz":-120,"elapsed":25974,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# create input variables. We only need <s, a, r> for REINFORCE\n","ph_states = tf.keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim, name=\"states\")\n","ph_actions = tf.keras.backend.placeholder(dtype='int32', name=\"action_ids\")\n","ph_cumulative_rewards = tf.keras.backend.placeholder(dtype='float32', name=\"cumulative_returns\")\n","ph_state_values = tf.keras.backend.placeholder(dtype='float32', name=\"state_values\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hip-IkAtifxO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1596823600492,"user_tz":-120,"elapsed":26367,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"ff6662e1-3238-4466-ff81-7b426ac4834a"},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Input\n","\n","#<YOUR CODE: define network graph using raw TF, Keras, or any other library you prefer>\n","\n","actor = Sequential([\n","        Dense(128, activation=\"relu\", input_shape=(None,) + state_dim),\n","        Dense(64, activation=\"relu\"),\n","        Dense(n_actions)\n","])\n","\n","critic = Sequential([\n","        Dense(128, activation=\"relu\", input_shape=(None,) + state_dim),\n","        Dense(64, activation=\"relu\"),\n","        Dense(1)\n","])\n","\n","logits_actions = actor(ph_states)\n","policy = tf.nn.softmax(logits_actions)\n","log_policy = tf.nn.log_softmax(logits_actions)\n","\n","V_critic = critic(ph_states)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (?, ?, 4) for input Tensor(\"dense_input:0\", shape=(?, ?, 4), dtype=float32), but it was called on an input with incompatible shape (?, 4).\n","WARNING:tensorflow:Model was constructed with shape (?, ?, 4) for input Tensor(\"dense_3_input:0\", shape=(?, ?, 4), dtype=float32), but it was called on an input with incompatible shape (?, 4).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fh4xmPuSifxY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823600493,"user_tz":-120,"elapsed":26350,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# Initialize model parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5Qcid3Lifxg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823600494,"user_tz":-120,"elapsed":26337,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    return policy.eval({ph_states: [states]})[0]"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hoaLHAK551Y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823600495,"user_tz":-120,"elapsed":26325,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["def predict_state_value(states):\n","    \"\"\" \n","    Predict state values given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, 1]\n","    \"\"\"\n","    return V_critic.eval({ph_states: [states]})[0]"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"El9Hu2TIifxq","colab_type":"text"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"TzBsRoOqifxr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823600495,"user_tz":-120,"elapsed":26311,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards, state_values = [], [], [], []\n","    s = env.reset()\n","\n","    pos_actions = list(range(n_actions))\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(s)\n","        v_s = predict_state_value(s)[0]\n","\n","        # Sample action with given probabilities.\n","        a = np.random.choice(pos_actions, p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","        state_values.append(v_s)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards, state_values"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYigX58Cifxy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1596823602152,"user_tz":-120,"elapsed":27953,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"cd3f4c7b-c245-4ea8-ba47-077fa1c2e29e"},"source":["# test it\n","states, actions, rewards, state_values = generate_session(env)\n","print(states[0])\n","print(actions)\n","print(rewards)\n","print(state_values)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[ 0.00061955 -0.02746257  0.03831938 -0.0179771 ]\n","[0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1]\n","[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","[-0.0010812961, -0.00598887, -0.012618724, -0.01870425, -0.023875095, -0.01461577, -0.00607178, -0.011082513, -0.0021241745, 0.007736674, 0.022022193, 0.0071749846, -0.004536923]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c2rfhfvWifx5","colab_type":"text"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$\n","\n","**Example**: For a trajectory $\\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_t, a_t, r_t, ..., s_T, a_T, r_T)$ the cumulative discounted rewards \n","$$R(\\tau) = \\sum_{i=0}^T \\gamma^i r_i \\approx \n","\\gamma^{T} V_\\phi(s_{T}) + \\sum_{i=0}^{T-1} \\gamma^i r_i$$ \n","where $V_\\phi$ is our critic."]},{"cell_type":"code","metadata":{"id":"wqM8l5Ltifx6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823602154,"user_tz":-120,"elapsed":27938,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["def get_cumulative_rewards(rewards,    # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    n = len(rewards)\n","    cum_reward = np.zeros(n).astype(float)\n","    cum_reward[n-1] = rewards[n-1]\n","\n","    for i in reversed(range(n-1)):\n","        cum_reward[i] = rewards[i] + gamma * cum_reward[i+1]\n","        \n","    return cum_reward #<array of cumulative rewards>"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDb0pYF8ifyB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596823602155,"user_tz":-120,"elapsed":27920,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"ece32362-be67-402a-d8a5-4ad42c9433a3"},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0EKJRBJ5ifyI","colab_type":"text"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"markdown","metadata":{"id":"pmHKXLKhxTbj","colab_type":"text"},"source":["### Theory\n","The cumulative discounted reward has a high variance and therefore **REINFORCE** needs lots of trajectories to converge.\n","In order to reduce it we can substract a baseline $b(s_t)$.\n","\n","This is possible because:\n","\n","$$\\mathbb{E}_{a_t \\sim \\pi_{\\theta}}{\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) b(s_t)} = 0.$$\n","\n","**Proof**\n","\n","Let $P_\\theta$ be the parameterized probability distribution over a random variable x.\n","$$\\int_x P_\\theta (x) = 1$$\n","Taking the gradient we get: $\\nabla_\\theta \\int_x P_\\theta (x) = \\nabla_\\theta 1 = 0$ and\n","$$\n","0 = \\nabla_\\theta \\int_x P_\\theta (x)\n","= \\int_x \\nabla_\\theta P_\\theta(x)\n","= \\int_x P_\\theta (x) \\nabla_\\theta \\log P_\\theta (x)\n","= \\mathbb{E}_{x \\sim P_\\theta} \\nabla_\\theta \\log P_\\theta (x)\n","$$\n","\n","Which leads to the following formula:\n","\n","$$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) \\left(\\sum_{t'=t}^T \\gamma^{t' - t} R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t)\\right)}$$\n","\n","The most common choice of baseline is the on-policy value function $V^{\\pi}(s_t)$. This choice has the desirable effect of reducing variance in the sample estimate for the policy gradient. This results in faster and more stable policy learning. It is also appealing from a conceptual angle: it encodes the intuition that if an agent gets what it expected, it should “feel” neutral about it.\n","\n","However $V^\\pi$ is unknown and therefore we need to learn it. We will use a neural network to approximate $V^\\pi$ with Mean Square Error as the loss function.\n","\n","$$ \\arg \\min_{\\phi} \\mathbb{E}_{s_t, \\hat{R}_t \\sim \\pi_k}{\\left( V_{\\phi}(s_t) - \\hat{R}_t \\right)^2}, $$\n","\n","\n","We can show that we can replace $\\sum_{t'=t}^T \\gamma^{t' - t} R(s_{t'}, a_{t'}, s_{t'+1})$ in the formula above by $Q^{\\pi_\\theta}(s_t, a_t)$ and by doing so we finally get $$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) \\left(Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t)\\right)}$$\n","\n","**Bonus: Prove the statement above**\n","\n","$A(s_t, a_t) = Q(s_{t}, a_{t}) - V^\\pi(s_t)$ is called the Advantage function which gives the name **Advantage Actor Critic**.\n","\n","$Q^{\\pi_\\theta}(s_t, a_t)$ is approximated as the cumulative sum of rewards.\n","\n","Minh et al. in their paper explained that they added an entropy term to the loss in order to encourage exploration.\n","\n","$$ - \\sum_{a} \\pi(a | s) \\log \\pi(a | s) $$\n","\n","**Q.6: Explain why adding the entropy term encourage exploration.**"]},{"cell_type":"code","metadata":{"id":"0r_SNoFnifyJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823602156,"user_tz":-120,"elapsed":27903,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n","log_policy_for_actions = tf.gather_nd(log_policy, indices)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"fS7qBwwUafBO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1596823602157,"user_tz":-120,"elapsed":27887,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"8cbeb812-ee7f-4223-8f27-0f68bebfe153"},"source":["print(indices)\n","print(log_policy_for_actions)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Tensor(\"stack:0\", shape=(?, 2), dtype=int32)\n","Tensor(\"GatherNd:0\", shape=(?,), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CNQWgVhaifyQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823602157,"user_tz":-120,"elapsed":27871,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n","# You may use log_policy_for_actions to get log probabilities for actions taken.\n","# Also recall that we defined ph_cumulative_rewards earlier.\n","\n","# Advantage\n","advantage = ph_cumulative_rewards - ph_state_values\n","\n","# Value - L2 squared loss\n","value_l2_squared_loss = tf.reduce_mean(tf.pow(advantage, 2))\n","\n","J = tf.reduce_mean(log_policy_for_actions * advantage)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kvye7UXrifyZ","colab_type":"text"},"source":["As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n","\n","$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"]},{"cell_type":"code","metadata":{"id":"W5qODJbgifya","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823602158,"user_tz":-120,"elapsed":27859,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n","# being deterministic, harming exploration.\n","\n","#entropy = <YOUR CODE: compute entropy. Do not forget the sign!>\n","entropy = tf.reduce_mean(-tf.reduce_sum(policy * log_policy, axis=-1))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"00L3TTY4ifyg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823602158,"user_tz":-120,"elapsed":27845,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# # Maximizing X is the same as minimizing -X, hence the sign.\n","loss = -(J + 0.1 * entropy) + value_l2_squared_loss\n","\n","update = tf.train.AdamOptimizer().minimize(loss)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Az5wj4Lyifym","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823602159,"user_tz":-120,"elapsed":27833,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["def train_on_session(states, actions, rewards, state_values):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    cumulative_rewards = get_cumulative_rewards(rewards)\n","    update.run({\n","        ph_states: states,\n","        ph_actions: actions,\n","        ph_cumulative_rewards: cumulative_rewards,\n","        ph_state_values: state_values\n","    })\n","    return np.sum(rewards)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"J61rD-8Vifyt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596823602159,"user_tz":-120,"elapsed":27819,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# Initialize optimizer parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bUEqCOFlify1","colab_type":"text"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"S87GZnO3ify3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":757},"executionInfo":{"status":"error","timestamp":1596825276073,"user_tz":-120,"elapsed":1701720,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"cd74e646-9283-4d8e-a7e1-b7061cef7d4d"},"source":["for i in range(50):\n","    \n","    c_rewards = []\n","    for j in range(100):\n","        states, actions, rewards, state_values = generate_session(env)\n","        r = train_on_session(states, actions, rewards, state_values)\n","        c_rewards.append(r)\n","    \n","    mean_reward = np.mean(c_rewards)\n","    print(\"mean reward: %.3f\" % (mean_reward))\n","\n","    if mean_reward >= 1000:\n","        print(\"You Win!!!\")  # but you can train even further\n","        break\n","print(\"You Win!\")"],"execution_count":22,"outputs":[{"output_type":"stream","text":["mean reward: 32.520\n","mean reward: 46.810\n","mean reward: 86.760\n","mean reward: 120.620\n","mean reward: 326.080\n","mean reward: 512.740\n","mean reward: 634.650\n","mean reward: 110.340\n","mean reward: 902.210\n","mean reward: 974.360\n","mean reward: 955.720\n","mean reward: 486.150\n","mean reward: 116.800\n","mean reward: 129.230\n","mean reward: 151.510\n","mean reward: 141.440\n","mean reward: 119.810\n","mean reward: 128.620\n","mean reward: 259.620\n","mean reward: 425.950\n","mean reward: 1000.000\n","mean reward: 1000.000\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-c9e5f70db63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mc_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mc_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-a5626d5a6b6b>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(env, t_max)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# action probabilities array aka pi(a|s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mv_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_state_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-fc798cae3295>\u001b[0m in \u001b[0;36mpredict_probs\u001b[0;34m(states)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mph_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \"\"\"\n\u001b[0;32m--> 913\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Use ref() instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5510\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5511\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5512\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"3yNdbw3hify-","colab_type":"text"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"5nn_GWABify_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596825328257,"user_tz":-120,"elapsed":48018,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXjNR-WOifzF","colab_type":"code","colab":{"resources":{"http://localhost:8080/videos/openaigym.video.0.125.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"status":404,"status_text":""}},"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1596825338113,"user_tz":-120,"elapsed":1170,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}},"outputId":"347cf1ee-1638-45b2-cc74-98ae9fc5725a"},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.125.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"gX0lvSu_rjJ-","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1596825276071,"user_tz":-120,"elapsed":1701674,"user":{"displayName":"Pavel Kolev","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgEXJS7sNFign0FwJAol1njlYVw7u2db6pzfVQUtA=s64","userId":"10114955063697182799"}}},"source":[""],"execution_count":null,"outputs":[]}]}