{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w9_TRPO_tf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SRyzt5S_75J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "965781b7-2a32-460e-8c62-1292edcd3d05"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    # https://github.com/yandexdataschool/Practical_RL/issues/256\n",
        "    !pip uninstall tensorflow --yes\n",
        "    !pip uninstall keras --yes\n",
        "    !pip install tensorflow-gpu==1.13.1\n",
        "    !pip install keras==2.2.4\n",
        "    \n",
        "    if not os.path.exists('.setup_complete'):\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "        #!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week08_pomdp/atari_util.py\n",
        "        \n",
        "        !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Uninstalling Keras-2.4.3:\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.31.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.18.5)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.35.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (49.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, keras-applications, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.2.4\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../xserver-common_2%3a1.19.6-1ubuntu4.5_all.deb ...\n",
            "Unpacking xserver-common (2:1.19.6-1ubuntu4.5) over (2:1.19.6-1ubuntu4.4) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.5_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.5) ...\n",
            "Setting up xserver-common (2:1.19.6-1ubuntu4.5) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQAUCblQ_75N",
        "colab_type": "text"
      },
      "source": [
        "### Let's make a TRPO!\n",
        "\n",
        "In this notebook we will write the code of the one Trust Region Policy Optimization.\n",
        "As usually, it contains a few different parts which we are going to reproduce.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzsdr5_8_75O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b61dfd2e-6082-47b1-e53e-d852973696cf"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHzq9yG7_75R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d7ad339b-1b82-494a-bb02-8ce9512c7555"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Acrobot-v1\")\n",
        "env.reset()\n",
        "\n",
        "observation_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Action Space\", env.action_space)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation Space Box(6,)\n",
            "Action Space Discrete(3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AS5Om-4_75T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "272def05-e455-40db-e7c0-704d60b30870"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f702877c978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgvVN9_z_75W",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Defining a network\n",
        "\n",
        "With all it's complexity, at it's core TRPO is yet another policy gradient method. \n",
        "\n",
        "This essentially means we're actually training a stochastic policy $ \\pi_\\theta(a|s) $. \n",
        "\n",
        "And yes, it's gonna be a neural network. So let's start by defining one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0sJTpTj_75X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# input tensors\n",
        "observations_ph = tf.placeholder(shape=(None, observation_shape[0]), \n",
        "                                 dtype=tf.float32)\n",
        "\n",
        "# Actions that we made\n",
        "actions_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "# \"G = r + gamma*r' + gamma^2*r'' + ...\"\n",
        "cummulative_returns_ph = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
        "\n",
        "# Action probabilities from previous iteration\n",
        "old_probs_ph = tf.placeholder(shape=(None, n_actions), dtype=tf.float32)\n",
        "\n",
        "all_inputs = [observations_ph, actions_ph,\n",
        "              cummulative_returns_ph, old_probs_ph]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOPs8OEn_75a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3f32359c-4662-4ebf-c72c-84d9407d6f29"
      },
      "source": [
        "def denselayer(name, x, out_dim, nonlinearity=None):\n",
        "    with tf.variable_scope(name):\n",
        "        if nonlinearity is None:\n",
        "            nonlinearity = tf.identity\n",
        "\n",
        "        W = tf.get_variable('W', shape=[x.shape[1], out_dim])\n",
        "        b = tf.get_variable('b', shape=[out_dim], \n",
        "                            initializer=tf.constant_initializer(0))\n",
        "        o = nonlinearity(tf.matmul(x, W) + b)\n",
        "\n",
        "        return o\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "\n",
        "# Policy\n",
        "nn = denselayer(\"policy_layer_1\", observations_ph, 16, tf.nn.relu)\n",
        "nn = denselayer(\"policy_layer_2\", nn, n_actions, None)\n",
        "\n",
        "policy_log   = tf.nn.log_softmax(nn)\n",
        "policy_probs = tf.exp(policy_log)\n",
        "\n",
        "train_vars = tf.trainable_variables()\n",
        "\n",
        "\n",
        "# State Value\n",
        "nn  = denselayer(\"value_layer_1\", observations_ph, 16, tf.nn.relu)\n",
        "V_s = denselayer(\"value_layer_2\", nn, 1, None)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auf2Au2__758",
        "colab_type": "text"
      },
      "source": [
        "### Step 3: loss functions\n",
        "\n",
        "Now let's define the loss functions and constraints for actual TRPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EkGpySi_759",
        "colab_type": "text"
      },
      "source": [
        "Advantage:\n",
        "$$A_{\\theta_{old}}(s_{i},a_{i})=G_{\\theta_{old}}(s_{i},a_{i})-V_{\\theta_{old}}(s_{i})$$\n",
        "\n",
        "where the gain function is\n",
        "$$G_{\\theta_{old}}(s_{i},a_{i})=\\sum_{j=i}^{T}\\gamma^{j-i}r_{j}$$\n",
        "\n",
        "The surrogate reward should be\n",
        "$$J_{surr}= {1 \\over N} \\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}\n",
        "A_{\\theta_{old}}(s_{i},a_{i})$$\n",
        "\n",
        "Clip Loss Function\n",
        "$$L_{\\theta_{old}}^{CLIP}(\\theta)=\\frac{1}{N}\\sum\\limits _{i=0}^{N}\\min\\left\\{ \\frac{\\pi_{\\theta}(s_{i},a_{i})}{\\pi_{\\theta_{old}}(s_{i},a_{i})}A_{\\theta_{old}}(s_{i},a_{i}),\\ g\\left(\\epsilon,A_{\\theta_{old}}(s_{i},a_{i})\\right)\\right\\}$$\n",
        "\n",
        "where function\n",
        "$$g(\\epsilon,A)=\\begin{cases}\n",
        "(1+\\epsilon)A & ,\\text{ if }A\\geq0\\\\\n",
        "(1-\\epsilon)A & ,\\text{ o.w.}\n",
        "\\end{cases}$$\n",
        "\n",
        "Or alternatively, minimize the surrogate loss:\n",
        "$$ L_{surr} = - L_{\\theta_{old}}^{CLIP}(\\theta) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0o3pphY_75-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select probabilities of chosen actions\n",
        "batch_size = tf.shape(policy_probs)[0]\n",
        "\n",
        "probs_all = tf.reshape(policy_probs, [-1])\n",
        "probs_for_actions = tf.gather(probs_all, \n",
        "                              tf.range(0, batch_size) * n_actions + actions_ph)\n",
        "\n",
        "old_probs_all = tf.reshape(old_probs_ph, [-1])\n",
        "old_probs_for_actions = tf.gather(old_probs_all, \n",
        "                                  tf.range(0, batch_size) * n_actions + actions_ph)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGzd6WwT_76B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute surrogate loss: negative importance-sampled policy gradient\n",
        "#L_surr = <YOUR CODE: compute surrogate loss, aka _negative_ importance-sampled policy gradient>\n",
        "\n",
        "ratio_ph     = probs_for_actions / old_probs_for_actions\n",
        "advantage_ph = cummulative_returns_ph - tf.stop_gradient(V_s[:,0])\n",
        "\n",
        "epsilon = 0.2\n",
        "\n",
        "# clipping pattern\n",
        "clippling_pattern = tf.minimum((1 - epsilon) * advantage_ph, 0) +\\\n",
        "                    tf.maximum((1 + epsilon) * advantage_ph, 0)\n",
        "\n",
        "L_surr = - tf.reduce_mean( tf.minimum(ratio_ph * advantage_ph, clippling_pattern) )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24ZPwx5ZwbwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Value loss\n",
        "V_loss = tf.reduce_mean((V_s[:,0] - tf.stop_gradient(cummulative_returns_ph))**2)\n",
        "\n",
        "train_V_loss = tf.train.AdamOptimizer(1e-4).minimize(V_loss)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imv9XYia_76F",
        "colab_type": "text"
      },
      "source": [
        "We can ascend these gradients as long as our $\\pi_\\theta(a|s)$ satisfies the constraint\n",
        "$$E_{s,\\pi_{\\Theta_{t}}}\\Big[KL(\\pi(\\Theta_{t}, s) \\:||\\:\\pi(\\Theta_{t+1}, s))\\Big] < \\alpha$$\n",
        "\n",
        "\n",
        "where\n",
        "\n",
        "$$KL(p||q) = E _p \\log\\left(\\frac{p}{q}\\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvAuUZZU_76G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all_inputs = [observations_ph, actions_ph,\n",
        "#              cummulative_returns_ph, old_probs_ph]\n",
        "# policy_log   = tf.nn.log_softmax(nn)\n",
        "# policy_probs = tf.exp(policy_log)\n",
        "\n",
        "# Compute Kullback-Leibler divergence (see formula above)\n",
        "# Note: you need to sum KL and entropy over all actions, not just the ones agent took\n",
        "old_log_probs = tf.log(old_probs_ph + 1e-10)\n",
        "new_log_probs = tf.log(policy_probs + 1e-10)\n",
        "\n",
        "b_size = tf.dtypes.cast(batch_size, tf.float32)\n",
        "\n",
        "#kl = <YOUR CODE: compute Kullback-Leibler as per formula above>\n",
        "kl = tf.reduce_sum(old_probs_ph * (old_log_probs - new_log_probs)) / b_size\n",
        "\n",
        "# Compute policy entropy\n",
        "#entropy = <YOUR CODE: compute policy entropy. Don't forget the sign!>\n",
        "entropy = - tf.reduce_sum(policy_probs * new_log_probs) / b_size\n",
        "\n",
        "# No variable depends on the following losses\n",
        "# Used only for progress tracking\n",
        "losses = [L_surr, kl, entropy]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BePcHv1K-u5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize TF\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d3IDLfp_76K",
        "colab_type": "text"
      },
      "source": [
        "**Linear search**\n",
        "\n",
        "TRPO in its core involves ascending surrogate policy gradient constrained by KL divergence. \n",
        "\n",
        "In order to enforce this constraint, we're gonna use linesearch. You can find out more about it [here](https://en.wikipedia.org/wiki/Linear_search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaMmk1QR_76L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linesearch(f, x, fullstep, max_kl):\n",
        "    \"\"\"\n",
        "    Linesearch finds the best parameters of neural networks \n",
        "    in the direction of fullstep contrainted by KL divergence.\n",
        "    :param: f - function that returns loss, kl and arbitrary third component.\n",
        "    :param: x - old parameters of neural network.\n",
        "    :param: fullstep - direction in which we make search.\n",
        "    :param: max_kl - constraint of KL divergence.\n",
        "    :returns:\n",
        "    \"\"\"\n",
        "    max_backtracks = 10\n",
        "    loss, _, _ = f(x)\n",
        "\n",
        "    step_frac = 1.0\n",
        "    for _ in np.arange(max_backtracks):\n",
        "        xnew = x + step_frac * fullstep\n",
        "        new_loss, kl, _ = f(xnew)\n",
        "\n",
        "        actual_improve = new_loss - loss\n",
        "        if kl <= max_kl and actual_improve < 0:\n",
        "            x = xnew\n",
        "            loss = new_loss\n",
        "            break\n",
        "            \n",
        "        step_frac *=  0.5\n",
        "    return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN1cdJZp_76D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "a9f39d11-3f35-40d6-ec25-ba33a4230aea"
      },
      "source": [
        "# compute and return surrogate policy gradient\n",
        "def var_shape(x):\n",
        "    res = [k.value for k in x.shape]\n",
        "    return res\n",
        "\n",
        "\n",
        "def numel(x):\n",
        "    return np.prod(var_shape(x))\n",
        "\n",
        "\n",
        "def flatgrad(loss, var_list):\n",
        "    grads = tf.gradients(loss, var_list)\n",
        "    print(grads)\n",
        "    grads_reshape = [tf.reshape(grad, [numel(v)])\n",
        "                     for (v, grad) in zip(var_list, grads)]\n",
        "\n",
        "    return tf.concat(grads_reshape, axis=0)\n",
        "\n",
        "\n",
        "flat_grad_L_surr = flatgrad(L_surr, train_vars)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "[<tf.Tensor 'gradients_1/policy_layer_1/MatMul_grad/MatMul_1:0' shape=(6, 16) dtype=float32>, <tf.Tensor 'gradients_1/policy_layer_1/add_grad/Reshape_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradients_1/policy_layer_2/MatMul_grad/MatMul_1:0' shape=(16, 3) dtype=float32>, <tf.Tensor 'gradients_1/policy_layer_2/add_grad/Reshape_1:0' shape=(3,) dtype=float32>]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p4yI2non7J_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47e516f4-52c8-49ab-d39c-c698b4703122"
      },
      "source": [
        "print(flat_grad_L_surr)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"concat:0\", shape=(163,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljE2-68L_75f",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Actions and rollouts\n",
        "\n",
        "In this section, we'll define functions that take actions $ a \\sim \\pi_\\theta(a|s) $ and rollouts $ \\langle s_0,a_0,s_1,a_1,s_2,a_2,...s_n,a_n \\rangle $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TaUC34w_75f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile function\n",
        "\n",
        "def act(obs, sample=True):\n",
        "    \"\"\"\n",
        "    Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n",
        "    :param: obs - single observation vector\n",
        "    :param sample: if True, samples from \\pi, otherwise takes most likely action\n",
        "    :returns: action (single integer) and probabilities for all actions\n",
        "    \"\"\"\n",
        "    # obs.reshape((1, -1)) makes batch first: [[obs]]\n",
        "    feed_dict = {observations_ph: obs.reshape((1, -1))}\n",
        "\n",
        "    probs = sess.run(policy_probs, feed_dict = feed_dict)[0]\n",
        "    \n",
        "    if sample:\n",
        "        action = int(np.random.choice(n_actions, p=probs))\n",
        "    else:\n",
        "        action = int(np.argmax(probs))\n",
        "\n",
        "    return action, probs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4WJ6clb_75l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2728d481-9d56-4409-dc5b-ecf015c8b7c4"
      },
      "source": [
        "# demo\n",
        "print(\"obs:\", env.reset())\n",
        "print()\n",
        "for _ in range(3):\n",
        "    print(\"sampled:\", act(env.reset()))\n",
        "    print(\"greedy:\", act(env.reset(), sample=False))\n",
        "    print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "obs: [ 0.9997294  -0.02326234  0.9999986   0.00167225  0.00454843  0.07938434]\n",
            "\n",
            "sampled: (1, array([0.2724268, 0.420629 , 0.3069442], dtype=float32))\n",
            "greedy: (1, array([0.27541646, 0.40993723, 0.31464627], dtype=float32))\n",
            "\n",
            "sampled: (1, array([0.27601695, 0.40527034, 0.31871265], dtype=float32))\n",
            "greedy: (1, array([0.27521372, 0.41136467, 0.3134216 ], dtype=float32))\n",
            "\n",
            "sampled: (1, array([0.27669567, 0.40993935, 0.31336495], dtype=float32))\n",
            "greedy: (1, array([0.27610815, 0.41095266, 0.31293923], dtype=float32))\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-0Sl5s8MbS1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "ccfce977-55fc-4d21-ff2e-be8d4169da45"
      },
      "source": [
        "# Initialize V_s\n",
        "obs = np.array([env.reset(), env.reset(), env.reset()])\n",
        "print(obs)\n",
        "feed_dict = {observations_ph: obs}\n",
        "state_value = sess.run(V_s, feed_dict = feed_dict)\n",
        "print(state_value[:,0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.99792621  0.06436831  0.99840324 -0.05648859  0.0633023   0.09996587]\n",
            " [ 0.99994569 -0.0104223   0.99995589 -0.00939209 -0.03606838  0.09570856]\n",
            " [ 0.99998175 -0.00604218  0.99996714 -0.00810684 -0.05910308 -0.01770903]]\n",
            "[-0.13778824 -0.19111481 -0.20929372]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX7Wf5kj_75o",
        "colab_type": "text"
      },
      "source": [
        "Compute cummulative reward just like you did in vanilla REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLl7xwjJ_75o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.signal\n",
        "\n",
        "def get_cummulative_returns(r, gamma=1):\n",
        "    \"\"\"\n",
        "    Computes cummulative discounted rewards given immediate rewards\n",
        "    G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
        "    Also known as R(s,a).\n",
        "    \"\"\"\n",
        "    r = np.array(r)\n",
        "    assert r.ndim >= 1\n",
        "    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NJY0SK8_75s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "661e2dd2-9818-4ee0-a756-919cca7414e6"
      },
      "source": [
        "# simple demo on rewards [0,0,1,0,0,1]\n",
        "print( get_cummulative_returns([0, 0, 1, 0, 0, 1], gamma=0.9) )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.40049 1.5561  1.729   0.81    0.9     1.     ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDLeSmTX_75w",
        "colab_type": "text"
      },
      "source": [
        "**Rollout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UFon-4g_75x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A valid path in a rollout must either:\n",
        "# end up in a \"done\" state or\n",
        "# exceed the allowed steps\n",
        "# NOTE: We might end up with a single path that exceeds steps limit !\n",
        "\n",
        "def rollout(env, act, max_pathlength=2500, n_timesteps=50000):\n",
        "    \"\"\"\n",
        "    Generate rollouts for training.\n",
        "    :param: env - environment in which we will make actions to generate rollouts.\n",
        "    :param: act - the function that can return policy and action given observation.\n",
        "    :param: max_pathlength - maximum size of one path that we generate.\n",
        "    :param: n_timesteps - total sum of sizes of all pathes we generate.\n",
        "    \"\"\"\n",
        "    paths = []\n",
        "\n",
        "    total_timesteps = 0\n",
        "\n",
        "    while total_timesteps < n_timesteps:\n",
        "        obervations, actions, rewards, action_probs = [], [], [], []\n",
        "        obervation = env.reset()\n",
        "        \n",
        "        for _ in range(max_pathlength):\n",
        "            action, policy_prob = act(obervation)\n",
        "\n",
        "            obervations.append(obervation)\n",
        "            actions.append(action)\n",
        "            action_probs.append(policy_prob)\n",
        "\n",
        "            obervation, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            total_timesteps += 1\n",
        "\n",
        "            if done or total_timesteps == n_timesteps:\n",
        "                path = {\"observations\": np.array(obervations),\n",
        "                        \"policy\":  np.array(action_probs),\n",
        "                        \"actions\": np.array(actions),\n",
        "                        \"rewards\": np.array(rewards),\n",
        "                        \"cumulative_returns\": get_cummulative_returns(rewards),\n",
        "                        }\n",
        "                paths.append(path)\n",
        "                break\n",
        "    # outputs List of Dictionaries (feed to nn)\n",
        "    return paths"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtubLIHvZmLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "paths = rollout(env, act, max_pathlength=5, n_timesteps=1000)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW-3i3_3ZrkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "94de5f5c-8f92-4874-caee-0e93e71d8ac3"
      },
      "source": [
        "print(len(paths))\n",
        "for path in paths:\n",
        "    for k,v in path.items():\n",
        "        print(k, len(v))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "observations 5\n",
            "policy 5\n",
            "actions 5\n",
            "rewards 5\n",
            "cumulative_returns 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQCZi6kQ_754",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "38d222d1-7a30-41f0-b745-9e1dc320132d"
      },
      "source": [
        "for k,v in paths[-1].items():\n",
        "    print(k, len(v))\n",
        "    print(v)\n",
        "\n",
        "assert (paths[0]['policy'].shape == (5, n_actions))\n",
        "assert (paths[0]['cumulative_returns'].shape == (5,))\n",
        "assert (paths[0]['rewards'].shape == (5,))\n",
        "assert (paths[0]['observations'].shape == (5,)+observation_shape)\n",
        "assert (paths[0]['actions'].shape == (5,))\n",
        "print('It\\'s ok')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observations 5\n",
            "[[ 0.9958774   0.09070945  0.99994289  0.01068687  0.0933941  -0.04415846]\n",
            " [ 0.99521475  0.09771179  0.99994998  0.01000213 -0.02436943  0.03831114]\n",
            " [ 0.99550725  0.09468533  0.9999587  -0.00908854 -0.00609196 -0.2262044 ]\n",
            " [ 0.99666424  0.08161118  0.99906969 -0.04312485 -0.12343287 -0.11065386]\n",
            " [ 0.99888722  0.04716273  0.99857927 -0.05328644 -0.21500257  0.00521211]]\n",
            "policy 5\n",
            "[[0.27287915 0.4195937  0.30752724]\n",
            " [0.27637205 0.41089284 0.31273517]\n",
            " [0.27412125 0.41192907 0.3139496 ]\n",
            " [0.27792707 0.40167904 0.32039374]\n",
            " [0.2825874  0.39405167 0.32336083]]\n",
            "actions 5\n",
            "[1 0 1 1 0]\n",
            "rewards 5\n",
            "[-1. -1. -1. -1. -1.]\n",
            "cumulative_returns 5\n",
            "[-5. -4. -3. -2. -1.]\n",
            "It's ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRRJZEk5_76O",
        "colab_type": "text"
      },
      "source": [
        "### Step 4: training\n",
        "In this section we construct rest parts of our computational graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No9-oU7O_76O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def slice_vector(vector, shapes):\n",
        "    \"\"\"\n",
        "    Slices symbolic vector into several symbolic tensors of given shapes.\n",
        "    Auxilary function used to un-flatten gradients, tangents etc.\n",
        "    :param vector: 1-dimensional symbolic vector\n",
        "    :param shapes: list or tuple of shapes (list, tuple or symbolic)\n",
        "    :returns: list of symbolic tensors of given shapes\n",
        "    \"\"\"\n",
        "    assert len(vector.shape) == 1, \"vector must be 1-dimensional\"\n",
        "    start = 0\n",
        "    tensors = []\n",
        "    for shape in shapes:\n",
        "        size = np.prod(shape)\n",
        "        tensor = tf.reshape(vector[start:(start + size)], shape)\n",
        "        tensors.append(tensor)\n",
        "        start += size\n",
        "    return tensors"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYlSxZxT_76R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "89d25d83-640d-4ff1-e4e3-b2cb529003d3"
      },
      "source": [
        "# (intermediate grad in conjugate_gradient)\n",
        "# \n",
        "# intended usage: -flat_grad = -derivative(L_surr)\n",
        "# this is the vector b in calling ConjugateGradient(F, b)\n",
        "conjugate_grad_intermediate_vector = tf.placeholder(dtype=tf.float32, shape=(None,))\n",
        "\n",
        "# slice flat_tangent into chunks for each weight\n",
        "weight_shapes = [sess.run(var).shape for var in train_vars]\n",
        "tangents = slice_vector(conjugate_grad_intermediate_vector, weight_shapes)\n",
        "\n",
        "# KL divergence where first arg is fixed\n",
        "kl_firstfixed = tf.reduce_sum((tf.stop_gradient(policy_probs) *\\\n",
        "                              (tf.stop_gradient(tf.log(policy_probs)) - tf.log(policy_probs)))\\\n",
        "                             ) / tf.cast(batch_size, tf.float32)      # The Gradient Part\n",
        "\n",
        "# MAIN OBSERVATION\n",
        "# H(f)*v = \\nabla_x( [\\nabla_x f(x)]^T * v )\n",
        "\n",
        "# compute fisher information matrix (used for conjugate gradients and to estimate KL)\n",
        "gradients = tf.gradients(kl_firstfixed, train_vars)\n",
        "gradient_vector_product = [tf.reduce_sum(g[0] * t) \n",
        "                           for (g, t) in zip(gradients, tangents)]\n",
        "\n",
        "# flatgrad - computes the second gradient!\n",
        "fisher_vec_prod = flatgrad(gradient_vector_product, train_vars)\n",
        "\n",
        "# Benefit: The Hessian matrix is not stored"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'gradients_3/policy_layer_1/MatMul_grad/MatMul_1:0' shape=(6, 16) dtype=float32>, <tf.Tensor 'gradients_3/policy_layer_1/add_grad/Reshape_1:0' shape=(16,) dtype=float32>, <tf.Tensor 'gradients_3/AddN_7:0' shape=(16, 3) dtype=float32>, <tf.Tensor 'gradients_3/policy_layer_2/add_grad/Reshape_1:0' shape=(3,) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykbwt6fy_76U",
        "colab_type": "text"
      },
      "source": [
        "### TRPO helpers\n",
        "\n",
        "Here we define a few helper functions used in the main TRPO loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDX_FDRL_76U",
        "colab_type": "text"
      },
      "source": [
        "**Conjugate gradients**\n",
        "\n",
        "Since TRPO includes contrainted optimization, we will need to solve Ax=b using conjugate gradients.\n",
        "\n",
        "In general, CG is an algorithm that solves Ax=b where A is positive-defined. A is Hessian matrix so A is positive-defined. You can find out more about them [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QTVWVc-_76V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
        "    \"\"\"\n",
        "    This method solves system of equation Ax=b using iterative method called conjugate gradients\n",
        "    :f_Ax: function that returns Ax\n",
        "    :b: targets for Ax\n",
        "    :cg_iters: how many iterations this method should do\n",
        "    :residual_tol: epsilon for stability\n",
        "    \"\"\"\n",
        "    eps = 1e-10\n",
        "\n",
        "    p = b.copy()\n",
        "    r = b.copy()\n",
        "    x = np.zeros_like(b)\n",
        "\n",
        "    rTr = r.dot(r)\n",
        "\n",
        "    for k in range(cg_iters):\n",
        "        Ap = f_Ax(p)\n",
        "        a_k = rTr / (p.dot(Ap) + eps)\n",
        "\n",
        "        x += a_k * p\n",
        "        r -= a_k * Ap\n",
        "\n",
        "        next_rTr = r.dot(r)\n",
        "        beta_k = next_rTr / (rTr + eps)\n",
        "\n",
        "        p = r + beta_k * p\n",
        "        rTr = next_rTr\n",
        "        \n",
        "        if rTr < residual_tol:\n",
        "            break\n",
        "    return x"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gb0C56t_76X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.linalg import inv\n",
        "\n",
        "k = 643\n",
        "\n",
        "# This code validates conjugate gradients\n",
        "A = np.random.rand(k, k)\n",
        "M = np.matmul(np.transpose(A), A)\n",
        "b = np.matmul(np.transpose(A), np.random.rand(k))\n",
        "#b = np.random.rand(k)\n",
        "\n",
        "def f_Mx(x):\n",
        "    return np.matmul(M, x.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "#(A^TA)^-1 A^T\n",
        "w = np.matmul(inv(M), b.reshape((-1, 1))).reshape(-1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC1ZBcFccbRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "604cf01d-56e7-4527-d209-f3ee58611653"
      },
      "source": [
        "print(w[:10])\n",
        "approx = conjugate_gradient(f_Mx, b, cg_iters=200)\n",
        "print(approx[:10])\n",
        "print(np.linalg.norm(w-approx, 2))\n",
        "print(np.matmul(np.transpose(w-approx), np.matmul(M,w-approx)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.3642384  -1.24401608  0.21617896 -1.06224863 -1.26000314 -2.01152717\n",
            "  0.07415452  2.61440875  1.23395269  0.40859281]\n",
            "[-0.10770018  0.39269919  0.28155797 -0.22494119 -0.40034862 -0.47322753\n",
            "  0.01625655  0.49178865  0.65867543 -0.02994356]\n",
            "25.765608227852987\n",
            "0.5823964356239977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnbjqLQj_76a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile a function that exports network weights as a vector\n",
        "flat_weights = tf.concat([tf.reshape(var, [-1]) for var in train_vars], axis=0)\n",
        "\n",
        "# ... and another function that imports vector back into network weights\n",
        "flat_weights_placeholder = tf.placeholder(tf.float32, shape=(None,))\n",
        "assigns = slice_vector(flat_weights_placeholder, weight_shapes)\n",
        "\n",
        "load_flat_weights = [w.assign(ph) for w, ph in zip(train_vars, assigns)]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrH9E9sk_76c",
        "colab_type": "text"
      },
      "source": [
        "##### Step 5: Main TRPO loop\n",
        "\n",
        "Here we will train our network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgJmJFz1_76d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76134512-620d-4643-ae6a-72de92db5425"
      },
      "source": [
        "import time\n",
        "from itertools import count\n",
        "from collections import OrderedDict\n",
        "\n",
        "# this is hyperparameter of TRPO. It controls how big KL divergence may be between old and new policy every step.\n",
        "max_kl = 0.01\n",
        "cg_damping = 0.1  # This parameters regularize addition to\n",
        "num_epis_total = 0    # number of played episodes\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "iters = 10\n",
        "for i in np.arange(iters):\n",
        "\n",
        "    print(\"\\n********** Iteration %i ************\" % (70 + i))\n",
        "\n",
        "\n",
        "    # Generating paths.\n",
        "    print(\"Rollout\")\n",
        "    paths = rollout(env, act)\n",
        "    print(\"Made rollout\")\n",
        "\n",
        "\n",
        "    # Load feed_dict and old_weights\n",
        "    observations = np.concatenate([path[\"observations\"] for path in paths])\n",
        "    actions      = np.concatenate([path[\"actions\"] for path in paths])\n",
        "    returns      = np.concatenate([path[\"cumulative_returns\"] for path in paths])\n",
        "    old_probs    = np.concatenate([path[\"policy\"] for path in paths])\n",
        "    \n",
        "    feed_dict = {observations_ph: observations,\n",
        "                 actions_ph: actions,\n",
        "                 old_probs_ph: old_probs,\n",
        "                 cummulative_returns_ph: returns,}\n",
        "    old_weights = sess.run(flat_weights)\n",
        "\n",
        "\n",
        "    # Fisher Vector Product\n",
        "    def Fvp(p):\n",
        "        \"\"\"gets intermediate grads (p) and computes (Fisher * vector) \"\"\"\n",
        "        feed_dict[conjugate_grad_intermediate_vector] = p\n",
        "        return sess.run(fisher_vec_prod, feed_dict) + cg_damping * p\n",
        "\n",
        "    flat_grad = sess.run(flat_grad_L_surr, feed_dict)\n",
        "\n",
        "    step_dir = conjugate_gradient(Fvp, -flat_grad, cg_iters=10)\n",
        "    alpha    = np.sqrt( 2 * max_kl / np.sum( step_dir * Fvp(step_dir) ) )\n",
        "    fullstep = alpha * step_dir\n",
        "\n",
        "\n",
        "    # Compute new weights with linesearch in the direction we found with CG\n",
        "\n",
        "    def losses_f(flat_weights):\n",
        "        feed_dict[flat_weights_placeholder] = flat_weights\n",
        "        sess.run(load_flat_weights, feed_dict)\n",
        "        return sess.run(losses, feed_dict)\n",
        "\n",
        "    new_weights = linesearch(losses_f, old_weights, fullstep, max_kl)\n",
        "    feed_dict[flat_weights_placeholder] = new_weights\n",
        "    sess.run(load_flat_weights, feed_dict)\n",
        "\n",
        "\n",
        "    # Train State Value loss\n",
        "    sess.run(train_V_loss, feed_dict = {observations_ph: observations,\n",
        "                                        cummulative_returns_ph: returns})\n",
        "\n",
        "    # Report current progress\n",
        "    L_surr, kl, entropy = sess.run(losses, feed_dict)\n",
        "    episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n",
        "\n",
        "    stats = OrderedDict()\n",
        "    num_epis_total += len(episode_rewards)\n",
        "\n",
        "    stats[\"Total number of episodes\"] = num_epis_total\n",
        "    stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n",
        "    stats[\"Std of rewards per episode\"] = episode_rewards.std()\n",
        "    stats[\"Entropy\"] = entropy\n",
        "    stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
        "    stats[\"KL between old and new distribution\"] = kl\n",
        "    stats[\"Surrogate loss\"] = L_surr\n",
        "\n",
        "    for k, v in stats.items():\n",
        "        print(k + \": \" + \" \" * (40 - len(k)) + str(v))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "********** Iteration 70 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 551\n",
            "Average sum of rewards per episode:       -89.74773139745916\n",
            "Std of rewards per episode:               30.16596364061745\n",
            "Entropy:                                  0.30406064\n",
            "Time elapsed:                             0.75 mins\n",
            "KL between old and new distribution:      0.00041284584\n",
            "Surrogate loss:                           49.4887\n",
            "\n",
            "********** Iteration 71 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 1098\n",
            "Average sum of rewards per episode:       -90.40950639853747\n",
            "Std of rewards per episode:               27.124072383795973\n",
            "Entropy:                                  0.29474753\n",
            "Time elapsed:                             1.49 mins\n",
            "KL between old and new distribution:      1.7707522e-10\n",
            "Surrogate loss:                           48.833366\n",
            "\n",
            "********** Iteration 72 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 1641\n",
            "Average sum of rewards per episode:       -91.08471454880295\n",
            "Std of rewards per episode:               32.96625047818919\n",
            "Entropy:                                  0.29931712\n",
            "Time elapsed:                             2.22 mins\n",
            "KL between old and new distribution:      -9.801375e-11\n",
            "Surrogate loss:                           51.05085\n",
            "\n",
            "********** Iteration 73 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 2191\n",
            "Average sum of rewards per episode:       -89.91090909090909\n",
            "Std of rewards per episode:               26.840628448998544\n",
            "Entropy:                                  0.30057818\n",
            "Time elapsed:                             2.95 mins\n",
            "KL between old and new distribution:      1.15971725e-10\n",
            "Surrogate loss:                           48.55118\n",
            "\n",
            "********** Iteration 74 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 2733\n",
            "Average sum of rewards per episode:       -91.25645756457564\n",
            "Std of rewards per episode:               35.83745984874711\n",
            "Entropy:                                  0.2933165\n",
            "Time elapsed:                             3.68 mins\n",
            "KL between old and new distribution:      1.0529886e-10\n",
            "Surrogate loss:                           52.156155\n",
            "\n",
            "********** Iteration 75 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 3285\n",
            "Average sum of rewards per episode:       -89.58333333333333\n",
            "Std of rewards per episode:               31.792242430252724\n",
            "Entropy:                                  0.30177367\n",
            "Time elapsed:                             4.41 mins\n",
            "KL between old and new distribution:      -1.3438957e-11\n",
            "Surrogate loss:                           49.998573\n",
            "\n",
            "********** Iteration 76 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 3847\n",
            "Average sum of rewards per episode:       -87.96975088967972\n",
            "Std of rewards per episode:               18.916678408272446\n",
            "Entropy:                                  0.3050498\n",
            "Time elapsed:                             5.14 mins\n",
            "KL between old and new distribution:      -2.0110659e-10\n",
            "Surrogate loss:                           45.62925\n",
            "\n",
            "********** Iteration 77 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 4409\n",
            "Average sum of rewards per episode:       -87.96975088967972\n",
            "Std of rewards per episode:               21.327912882170967\n",
            "Entropy:                                  0.30299422\n",
            "Time elapsed:                             5.87 mins\n",
            "KL between old and new distribution:      1.6367467e-10\n",
            "Surrogate loss:                           46.1392\n",
            "\n",
            "********** Iteration 78 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 4957\n",
            "Average sum of rewards per episode:       -90.24270072992701\n",
            "Std of rewards per episode:               27.341004231997168\n",
            "Entropy:                                  0.30372694\n",
            "Time elapsed:                             6.62 mins\n",
            "KL between old and new distribution:      2.0201929e-11\n",
            "Surrogate loss:                           48.84592\n",
            "\n",
            "********** Iteration 79 ************\n",
            "Rollout\n",
            "Made rollout\n",
            "Total number of episodes:                 5512\n",
            "Average sum of rewards per episode:       -89.0936936936937\n",
            "Std of rewards per episode:               29.47605924031584\n",
            "Entropy:                                  0.2992322\n",
            "Time elapsed:                             7.34 mins\n",
            "KL between old and new distribution:      -1.1575963e-10\n",
            "Surrogate loss:                           48.944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_T5qDKCx17K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e69d043b-c3f1-4618-b989-81507de64fbb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = [np.sum(p['rewards']) for p in paths]\n",
        "plt.hist(x, bins = 50)\n",
        "plt.show()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAP1klEQVR4nO3df4xlZX3H8fenbNFqY3dxpxR3SWdbN7Rof0gmlMakMWLLCsaliTUYo6vSbE1BrdrgIomYEpOltqXaHzSrUNeEoIRq2Ai2bqnENCnogID80DLlh+wGZAz+aGqiXf32j3us13F2Z+aeOzM7z7xfyWTOec5z7vk+u3c/e+Y595xJVSFJastPrXYBkqTxM9wlqUGGuyQ1yHCXpAYZ7pLUoA2rXQDA5s2ba3JycrXLkKQ15c477/x6VU3Mt+24CPfJyUmmp6dXuwxJWlOSPHa0bU7LSFKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg46LO1QlaRSTe26et/3RveetcCXHH8/cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ1aMNyTXJvkqST3zbPtnUkqyeZuPUk+mGQmyb1JzliOoiVJx7aYM/ePADvmNiY5Ffg94KtDzS8Htndfu4Gr+5coSVqqBcO9qj4HPD3PpquAS4AaatsJfLQGbgc2JjllLJVKkhZtpDn3JDuBw1V1z5xNW4DHh9YPdW3zvcbuJNNJpmdnZ0cpQ5J0FEsO9yTPAt4NvKfPgatqX1VNVdXUxMREn5eSJM0xylMhfxnYBtyTBGArcFeSM4HDwKlDfbd2bZKkFbTkM/eq+lJV/XxVTVbVJIOplzOq6kngAPD67lMzZwHfqqonxluyJGkhi/ko5PXAfwCnJTmU5MJjdL8FeBiYAT4E/PFYqpQkLcmC0zJV9ZoFtk8OLRdwUf+yJEl9eIeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNGuXxA5K0oib33LzaJaw5nrlLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNWswvyL42yVNJ7htqe3+SLye5N8knk2wc2nZpkpkkX0lyznIVLkk6usWcuX8E2DGn7SDwwqr6deA/gUsBkpwOXAC8oNvn75OcMLZqJUmLsmC4V9XngKfntH2mqo50q7cDW7vlncDHquq7VfUIMAOcOcZ6JUmLMI459zcBn+6WtwCPD2071LX9hCS7k0wnmZ6dnR1DGZKkH+oV7kkuA44A1y1136raV1VTVTU1MTHRpwxJ0hwj/7KOJG8AXgGcXVXVNR8GTh3qtrVrkyStoJHO3JPsAC4BXllV3xnadAC4IMkzkmwDtgOf71+mJGkpFjxzT3I98BJgc5JDwOUMPh3zDOBgEoDbq+rNVXV/khuABxhM11xUVd9fruIlSfNbMNyr6jXzNF9zjP7vA97XpyhJUj/eoSpJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBo38PHdJOl5N7rl53vZH9563wpWsHs/cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMWDPck1yZ5Ksl9Q20nJTmY5KHu+6auPUk+mGQmyb1JzljO4iVJ81vMmftHgB1z2vYAt1bVduDWbh3g5cD27ms3cPV4ypQkLcWC4V5VnwOentO8E9jfLe8Hzh9q/2gN3A5sTHLKuIqVJC3OqHPuJ1fVE93yk8DJ3fIW4PGhfoe6tp+QZHeS6STTs7OzI5YhSZpP7wuqVVVAjbDfvqqaqqqpiYmJvmVIkoaMGu5f++F0S/f9qa79MHDqUL+tXZskaQWNGu4HgF3d8i7gpqH213efmjkL+NbQ9I0kaYUs+FTIJNcDLwE2JzkEXA7sBW5IciHwGPDqrvstwLnADPAd4I3LULMkaQELhntVveYom86ep28BF/UtSpLUj3eoSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqUK9wT/L2JPcnuS/J9UmemWRbkjuSzCT5eJITx1WsJGlxRg73JFuAtwJTVfVC4ATgAuBK4Kqqej7wDeDCcRQqSVq8vtMyG4CfSbIBeBbwBPBS4MZu+37g/J7HkCQt0cjhXlWHgb8Avsog1L8F3Al8s6qOdN0OAVv6FilJWpo+0zKbgJ3ANuB5wLOBHUvYf3eS6STTs7Ozo5YhSZpHn2mZlwGPVNVsVf0v8AngxcDGbpoGYCtweL6dq2pfVU1V1dTExESPMiRJc/UJ968CZyV5VpIAZwMPAJ8FXtX12QXc1K9ESdJS9Zlzv4PBhdO7gC91r7UPeBfwjiQzwHOBa8ZQpyRpCTYs3OXoqupy4PI5zQ8DZ/Z5XUlSP96hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDWoV7gn2ZjkxiRfTvJgkt9OclKSg0ke6r5vGlexkqTF6Xvm/gHgn6vqV4DfAB4E9gC3VtV24NZuXZK0gkYO9yQ/B/wOcA1AVX2vqr4J7AT2d932A+f3LVKStDR9zty3AbPAPyb5YpIPJ3k2cHJVPdH1eRI4eb6dk+xOMp1kenZ2tkcZkqS5+oT7BuAM4OqqehHwP8yZgqmqAmq+natqX1VNVdXUxMREjzIkSXP1CfdDwKGquqNbv5FB2H8tySkA3fen+pUoSVqqkcO9qp4EHk9yWtd0NvAAcADY1bXtAm7qVaEkack29Nz/LcB1SU4EHgbeyOA/jBuSXAg8Bry65zEkSUvUK9yr6m5gap5NZ/d5XUlSP96hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg/rexCRJYzO55+bVLqEZhrukdeNo/3k8uve8Fa5k+TktI0kNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBvcM9yQlJvpjkU936tiR3JJlJ8vHul2dLklbQOM7c3wY8OLR+JXBVVT0f+AZw4RiOIUlagl7hnmQrcB7w4W49wEuBG7su+4Hz+xxDkrR0fc/c/xq4BPhBt/5c4JtVdaRbPwRsmW/HJLuTTCeZnp2d7VmGJGnYyOGe5BXAU1V15yj7V9W+qpqqqqmJiYlRy5AkzaPP89xfDLwyybnAM4HnAB8ANibZ0J29bwUO9y9TkrQUI5+5V9WlVbW1qiaBC4B/q6rXAp8FXtV12wXc1LtKSdKSLMfn3N8FvCPJDIM5+GuW4RiSpGMYy6/Zq6rbgNu65YeBM8fxupKk0XiHqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgkcM9yalJPpvkgST3J3lb135SkoNJHuq+bxpfuZKkxehz5n4EeGdVnQ6cBVyU5HRgD3BrVW0Hbu3WJUkraORwr6onququbvm/gQeBLcBOYH/XbT9wft8iJUlLM5Y59ySTwIuAO4CTq+qJbtOTwMlH2Wd3kukk07Ozs+MoQ5LU2dD3BZL8LPBPwJ9U1beT/P+2qqokNd9+VbUP2AcwNTU1bx9JbZrcc/Nql9C8XmfuSX6aQbBfV1Wf6Jq/luSUbvspwFP9SpQkLVWfT8sEuAZ4sKr+amjTAWBXt7wLuGn08iRJo+gzLfNi4HXAl5Lc3bW9G9gL3JDkQuAx4NX9SpQkLdXI4V5V/w7kKJvPHvV1JUn9eYeqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoN7Pc5eko/G57avHM3dJapDhLkkNMtwlqUHOuUvr1NHmwx/de94KV6LlYLhLjfOi5vrktIwkNcgzd0mL5lTO2rFs4Z5kB/AB4ATgw1W1d7mOJY3DcgfXUqdH1lJgOvVz/FmWcE9yAvB3wO8Ch4AvJDlQVQ+M+1jHelMt9z8Oz2JW3/H4dzCuoFut/wwM6uWx0u/V5ZpzPxOYqaqHq+p7wMeAnct0LEnSHKmq8b9o8ipgR1X9Ybf+OuC3qurioT67gd3d6mnAV0Y83Gbg6z3KXasc9/riuNeXxY77F6tqYr4Nq3ZBtar2Afv6vk6S6aqaGkNJa4rjXl8c9/oyjnEv17TMYeDUofWtXZskaQUsV7h/AdieZFuSE4ELgAPLdCxJ0hzLMi1TVUeSXAz8C4OPQl5bVfcvx7EYw9TOGuW41xfHvb70n7JejguqkqTV5eMHJKlBhrskNWhNhXuS9yY5nOTu7uvcoW2XJplJ8pUk5wy17+jaZpLsWZ3KxyPJO5NUks3depJ8sBvbvUnOGOq7K8lD3deu1at6dEmu6MZ1d5LPJHle1976uN+f5Mvd2D6ZZOPQtmbf50n+IMn9SX6QZGrOtmbHPdfYxlRVa+YLeC/wp/O0nw7cAzwD2Ab8F4MLuSd0y78EnNj1OX21xzHi2E9lcIH6MWBz13Yu8GkgwFnAHV37ScDD3fdN3fKm1R7DCGN+ztDyW4F/WCfj/j1gQ7d8JXBlt9z0+xz4VQY3NN4GTA21Nz3uOX8GYxvTmjpzP4adwMeq6rtV9Qgww+ARCC09BuEq4BJg+Ar4TuCjNXA7sDHJKcA5wMGqerqqvgEcBHaseMU9VdW3h1afzY/G3vq4P1NVR7rV2xncJwKNv8+r6sGqmu9O9abHPcfYxrQWw/3i7sfVa5Ns6tq2AI8P9TnUtR2tfU1JshM4XFX3zNnU9LgBkrwvyePAa4H3dM3Nj3vImxj8lALra9zD1tO4xzam4+557kn+FfiFeTZdBlwNXMHgDO4K4C8ZvPnXvAXG/W4GP6o351jjrqqbquoy4LIklwIXA5evaIHLZKFxd30uA44A161kbctpMePWeBx34V5VL1tMvyQfAj7VrR7rcQdr4jEIRxt3kl9jMM94TxIYjOGuJGdy9HEfBl4yp/22sRc9Bov9+2YQcLcwCPfmx53kDcArgLOrm4yl4ff5Atb8uJdgfI9uWe0LCEu82HDK0PLbGczDAbyAH7/g8jCDCxMbuuVt/OjixAtWexw9/wwe5UcXVM/jxy8sfr5rPwl4hMFFxU3d8kmrXfsIY90+tPwW4MZ1Mu4dwAPAxJz2dfE+5ycvqK6LcXdjHduYjrsz9wX8eZLfZDAt8yjwRwBVdX+SGxj8gzgCXFRV3wdYwccgrIZbGHxyZAb4DvBGgKp6OskVDJ7xA/BnVfX06pTYy94kpwE/YPApoTd37a2P+28ZBNnB7qe126vqza2/z5P8PvA3wARwc5K7q+qc1sc9rMb46BYfPyBJDVqLn5aRJC3AcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkN+j9pqZXH/Oo+cQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzjIK8BoAP3t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02ca6523-6022-4897-903c-9026071719a5"
      },
      "source": [
        "paths = rollout(env, act)\n",
        "print(np.mean([np.sum(p['rewards']) for p in paths]))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-91.08471454880295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KXLiWZYben0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(env, n_games=1):\n",
        "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
        "\n",
        "    game_rewards = []\n",
        "    for _ in range(n_games):\n",
        "        # initial observation and memory\n",
        "        observation = env.reset()\n",
        "\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action, _ = act(observation)\n",
        "            observation, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        game_rewards.append(total_reward)\n",
        "    return game_rewards"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6bjqpBtl9_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08248321-d1d5-443f-965d-61a02720d416"
      },
      "source": [
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(env, directory=\"videos\", force=True) as env_monitor:\n",
        "    final_rewards = evaluate(env_monitor, n_games=5)\n",
        "\n",
        "print(\"Final rewards\", final_rewards)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final rewards [-90.0, -63.0, -113.0, -77.0, -90.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IXb-scZdj2I",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.39.99.video000001.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "5e850c69-b366-4ef9-f5e1-19d35998c81b"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.39.99.video000001.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6mISEoL_76h",
        "colab_type": "text"
      },
      "source": [
        "# Homework option I: better sampling (10+pts)\n",
        "\n",
        "In this section, you're invited to implement a better rollout strategy called _vine_.\n",
        "\n",
        "![img](https://s17.postimg.cc/i90chxgvj/vine.png)\n",
        "\n",
        "In most gym environments, you can actually backtrack by using states. You can find a wrapper that saves/loads states in [the mcts seminar](https://github.com/yandexdataschool/Practical_RL/blob/master/week10_planning/seminar_MCTS.ipynb).\n",
        "\n",
        "You can read more about in the [TRPO article](https://arxiv.org/abs/1502.05477) in section 5.2.\n",
        "\n",
        "The goal here is to implement such rollout policy (we recommend using tree data structure like in the seminar above).\n",
        "Then you can assign cummulative rewards similar to `get_cummulative_rewards`, but for a tree.\n",
        "\n",
        "__bonus task__ - parallelize samples using multiple cores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khJsr6IU_76h",
        "colab_type": "text"
      },
      "source": [
        "# Homework option II (10+pts)\n",
        "\n",
        "Let's use TRPO to train evil robots! (pick any of two)\n",
        "* [MuJoCo robots](https://gym.openai.com/envs#mujoco)\n",
        "* [Box2d robot](https://gym.openai.com/envs/BipedalWalker-v2)\n",
        "\n",
        "The catch here is that those environments have continuous action spaces. \n",
        "\n",
        "Luckily, TRPO is a policy gradient method, so it's gonna work for any parametric $\\pi_\\theta(a|s)$. We recommend starting with gaussian policy:\n",
        "\n",
        "$$\\pi_\\theta(a|s) = N(\\mu_\\theta(s),\\sigma^2_\\theta(s)) = {1 \\over \\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) } } e^{ (a - \n",
        "\\mu_\\theta(s))^2 \\over 2 {\\sigma^2}_\\theta(s) } $$\n",
        "\n",
        "In the $\\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) }$ clause, $\\pi$ means ~3.1415926, not agent's policy.\n",
        "\n",
        "This essentially means that you will need two output layers:\n",
        "* $\\mu_\\theta(s)$, a dense layer with linear activation\n",
        "* ${\\sigma^2}_\\theta(s)$, a dense layer with activation tf.exp (to make it positive; like rho from bandits)\n",
        "\n",
        "For multidimensional actions, you can use fully factorized gaussian (basically a vector of gaussians).\n",
        "\n",
        "__bonus task__: compare performance of continuous action space method to action space discretization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z44qcI-y_76i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}